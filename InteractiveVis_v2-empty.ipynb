{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the interactivity I'm using _plotly_ library and _Jupyter Widgets_. \n",
    "It allows to prototype dashboard in some way. Hence, for correct execution of the current notebook one has to install the following packages: \n",
    "   - [plotly](https://plot.ly/python/) + [cufflinks](https://plot.ly/ipython-notebooks/cufflinks/)\n",
    "   - [ipywidgets](https://github.com/jupyter-widgets/ipywidgets)\n",
    "\n",
    "and activate the required extensions for Jupyter. Feel free to do it manually or run the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install plotly\n",
    "# pip install cufflinks\n",
    "# pip install ipywidgets\n",
    "# jupyter nbextension install --py --sys-prefix widgetsnbextension\n",
    "# jupyter nbextension install --py --sys-prefix plotlywidget\n",
    "# jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "# jupyter nbextension enable --py --sys-prefix plotlywidget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the correct installation and activation of the required packages please execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "jupyter nbextension list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data I/O\n",
    "We assume that one has already obtained the embeddings for the researched financial statement network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from NetEmbs import *\n",
    "\n",
    "MODE = \"RealData\"\n",
    "\n",
    "\n",
    "if MODE == \"SimulatedData\":\n",
    "    EMBS_PATH = \"/Users/mboersma/Documents/phd/students/alex/NetEmbs-dev-12/model/15108_2017_versionMetaDiff_directionCOMBI_walks31_pressure30_window3/TFsteps100000batch64_emb32\"\n",
    "    embs = pd.read_pickle(EMBS_PATH+\"cache/Embeddings.pkl\")\n",
    "    print(\"Embeddings have been uploaded to memory!\")\n",
    "    d = upload_data(\"Simulation/FSN_Data.db\", limit=None)\n",
    "    d = prepare_data(d)\n",
    "    print(\"Supported information has been uploaded to memory!\")\n",
    "\n",
    "if MODE == \"RealData\":\n",
    "    import extras\n",
    "    import analysis\n",
    "    EMBS_PATH = \"model/15108_2017_versionMetaDiff_directionCOMBI_walks31_pressure30_window3/TFsteps100000batch64_emb32/\"\n",
    "    embs = pd.read_pickle(EMBS_PATH+\"cache/Embeddings.pkl\")\n",
    "        # //////// TODO UPLOAD your data HERE \\\\\\\\\\\\\\\\\\\\\n",
    "#     d = analysis.analysis(\"14082_2017\")\n",
    "    d = extras.getData(\"15108_2017\")\n",
    "        # //////// END  \\\\\\\\\\\\\\\\\\\\\n",
    "    # TODO pay attention for the split argument below!\n",
    "    if \"Value\" in list(d):\n",
    "        need_split = True\n",
    "    else:\n",
    "        need_split = False\n",
    "    d = prepare_dataMarcel(d, split=need_split)\n",
    "#     Here we drop the duplicate of GroundTruth in the DataFrame with supported info, because we have it in Embs DataFrame\n",
    "    if \"GroundTruth\" in list(d):\n",
    "        d.drop(\"GroundTruth\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interactive visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Decrease the dim of embeddings for visualization purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "def dim_reduction(df, n_dim=2, rand_state=1):\n",
    "    if \"Emb\" in list(df):\n",
    "        tsne = TSNE(random_state=rand_state)\n",
    "        embdf = pd.DataFrame(list(map(np.ravel, df[\"Emb\"])))\n",
    "        embed_tsne = tsne.fit_transform(embdf)\n",
    "        df[\"x\"] = pd.Series(embed_tsne[:, 0])\n",
    "        df[\"y\"] = pd.Series(embed_tsne[:, 1])\n",
    "        return df\n",
    "    else:\n",
    "        raise KeyError(\"No Embs column in the given DataFrame!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# Standard plotly imports\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, plot, init_notebook_mode\n",
    "# Using plotly + cufflinks in offline mode\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)\n",
    "from ipywidgets import interactive, HBox, VBox, widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "# Count most frequent FA names in the given DataFrame OR FA names with the highest amount\n",
    "def findMostCommonFAs_v2(df, labels_column=\"label\", words_column=\"FA_Name\", amount_column=\"amount\", sort_mode=\"freq\", n_top=4, vis=False, folder=\"\"):\n",
    "    if labels_column not in list(df):\n",
    "        raise KeyError(f\"Given column name {labels_column} is not presented in the given DataFrame! Only allows: {list(df)}!\")\n",
    "    if \"from\" not in list(df):\n",
    "        raise KeyError(f\"Please ensure that column 'from' is presented in your DataFrame!\")\n",
    "    for name, group in df.groupby(labels_column):\n",
    "        print(\"Current cluster label is \", name)\n",
    "        gr = group.groupby([words_column, \"from\"])\n",
    "        counts = gr.size().to_frame(name='counts')\n",
    "        all_stat = counts.join(gr.agg({amount_column: sum, 'Debit': lambda x: list(x), 'Credit': lambda x: list(x)})\n",
    "              .rename(columns={amount_column: 'amount_sum', 'Debit': 'Debit_list', 'Credit': 'Credit_list'}))\\\n",
    "        .reset_index()\n",
    "        if sort_mode == \"freq\":\n",
    "            all_stat.sort_values(['counts', words_column], ascending=False, inplace=True)\n",
    "        elif sort_mode == \"amount\":\n",
    "            all_stat.sort_values(['amount_sum', word_column], ascending=False, inplace=True)\n",
    "#             Store all statistict for N_TOP values as dictionary for further visualization\n",
    "        text = {\"Left\": [(x[0], x[2], x[3], x[5]) for x in all_stat[all_stat[\"from\"]==True].values[:n_top]], \n",
    "                \"Right\": [(x[0], x[2], x[3], x[4]) for x in all_stat[all_stat[\"from\"]==False].values[:n_top]]}\n",
    "        if vis:\n",
    "            i = 0\n",
    "            fig, axes = plt.subplots(2,2)\n",
    "        for key, data in text.items():\n",
    "            if sort_mode == \"freq\":\n",
    "#             Take the most frequent FA names\n",
    "                to_vis = [(str(item[0]), item[1]) for item in data]\n",
    "            elif sort_mode == \"amount\":\n",
    "                to_vis = [(str(item[0]), item[2]) for item in data]\n",
    "            print(key, \"--->\", [item[:3] for item in data])\n",
    "            if vis:\n",
    "#                 WordClouds\n",
    "                axes[0, i].set_title(key, size=24)\n",
    "                wc = WordCloud(background_color=\"white\", width=800, height=400, max_font_size=84, min_font_size=14, repeat=False, relative_scaling=0.8, max_words=100)\n",
    "                if len(to_vis)>0:\n",
    "                    wc.generate_from_frequencies(dict(to_vis))\n",
    "                else:\n",
    "                    continue\n",
    "                axes[0, i].axis(\"off\")\n",
    "                axes[0, i].imshow(wc, interpolation=\"bilinear\")\n",
    "#                 Histograhm\n",
    "                [sns.distplot( item[3] , label=item[0], kde=False, bins=50, ax=axes[1, i], hist_kws={\"range\": (0, 1.0)}) for item in data if len(item[3])>4]\n",
    "                axes[1,i].legend(frameon=False, fontsize=14)\n",
    "                axes[1,i].set_xlim((0,1.0))\n",
    "                i+=1\n",
    "        if vis:\n",
    "            plt.tight_layout()\n",
    "#             plt.savefig(folder + \"img/WordClouds/\" + str(name), dpi=140, pad_inches=0.01)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Matplotlib colormap into plotly colorscale:\n",
    "import itertools\n",
    "def matplotlib_to_plotly(color_map=\"tab10\", pl_entries=10):\n",
    "    cmap = matplotlib.cm.get_cmap(color_map)\n",
    "    h = 1.0/(pl_entries-1)\n",
    "    pl_colorscale = []\n",
    "\n",
    "    for k in range(pl_entries):\n",
    "        C = list(map(np.uint8, np.array(cmap(k*h)[:3])*255))\n",
    "        pl_colorscale.append([k*h, 'rgb'+str((C[0], C[1], C[2]))])\n",
    "\n",
    "    return pl_colorscale\n",
    "\n",
    "def getColors_Markers(keys, cm=\"tab10\", n_color=10, markers = [\"circle\", \"diamond\", \"square\"]):\n",
    "    keys = sorted(keys)\n",
    "    color_map = dict(zip(keys, matplotlib_to_plotly(cm, n_color)*(len(keys)//n_color+1)))\n",
    "    marker_map = dict(zip(keys, list(itertools.chain(*[[m]*n_color for m in markers]))*(len(keys)//(3*n_color)+1)))\n",
    "    return color_map, marker_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLS = 8\n",
    "embs = dim_reduction(cl_Agglomerative(embs, N_CLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO before visualization one has to use t-SNE!\n",
    "# Label text\n",
    "description = widgets.Label(\n",
    "        value=''\n",
    "    )\n",
    "# WordCouds area\n",
    "wordCloudsOutput = widgets.Output()\n",
    "# Table with JournalEntries data\n",
    "table_titles = [\"ID\", \"FA_Name\", \"Credit\", \"Debit\", \"label\"]\n",
    "\n",
    "if MODE == \"RealData\":\n",
    "    table_titles = [\"ID\", \"FA_Name\", \"accountDesc\", \"Credit\", \"Debit\", \"label\"]\n",
    "    \n",
    "t = go.FigureWidget([go.Table(\n",
    "    header=dict(values=table_titles,\n",
    "                fill = dict(color='#C2D4FF'),\n",
    "                align = ['left'] * 5),\n",
    "    cells=dict(values=[],\n",
    "               fill = dict(color='#F5F8FF'),\n",
    "               align = ['left'] * 5))],\n",
    "                    layout = go.Layout(\n",
    "                            title=\"Journal Entries\",\n",
    "                            autosize=True,\n",
    "                            width=1000,\n",
    "                            height=400))\n",
    "# Scatter plot\n",
    "N_COLORS = 10\n",
    "WORD_CLOUD_LABEL = \"FA_Name\"\n",
    "# LEGEND_TITLE = \"GroundTruth\"\n",
    "LEGEND_TITLE = \"label\"\n",
    "tmp_p_see = None\n",
    "# For selection via multiple traces... stupid way.\n",
    "indexes = []\n",
    "tr_nums = 0\n",
    "\n",
    "\n",
    "def interactiveScatter(df, df_info, legend_title=\"label\"):\n",
    "    \"\"\"Create FigureWidget with the scatter plot for the given DataFrame\"\"\"\n",
    "    scatter_data = list()\n",
    "    cmap, mmap = getColors_Markers(keys=df[legend_title].unique(), n_color=N_COLORS)\n",
    "    for name, group in df.groupby(legend_title):\n",
    "        scatter_data.append(go.Scatter(x=group.x, y=group.y, mode='markers', name=name, \n",
    "                                    text = group.apply(lambda row: f\"ID={row.ID},   GroundTruth={row.GroundTruth}\", axis=1),\n",
    "                                    customdata = group.index.to_list(),\n",
    "                                    marker=dict(color=cmap[name][1], \n",
    "                                                symbol=mmap[name])))\n",
    "    f = go.FigureWidget(data=scatter_data,\n",
    "                       layout = go.Layout(\n",
    "                           title=f\"t-SNE visualisation with coloring based on {legend_title}\",\n",
    "        hovermode='closest',\n",
    "        autosize=True,\n",
    "        width=1000,\n",
    "        height=700))\n",
    "    \n",
    "    def printSignature(trace, points, *args):\n",
    "        if len(points.point_inds)>0:\n",
    "            ids = trace.customdata[points.point_inds[0]]\n",
    "            row = df.iloc[ids]\n",
    "            description.value = f\"ID={row.ID},   GroundTruth={row.GroundTruth}\"\n",
    "    def selectBP(trace, points, *args):\n",
    "        if len(points.point_inds)>0:\n",
    "            ids = trace.customdata[points.point_inds[0]]\n",
    "            row = df.iloc[[ids]]\n",
    "            chosen_bps = df_info.merge(row, on=\"ID\")\n",
    "            t.data[0].cells.values = [chosen_bps[col] for col in t.data[0].header.values]\n",
    "    \n",
    "    def filterRows(selected_ids):\n",
    "        row = df.iloc[selected_ids]\n",
    "        chosen_bps = df_info.merge(row, on=\"ID\")\n",
    "        return chosen_bps\n",
    "    def updateTable(chosen_bps):\n",
    "        t.data[0].cells.values = [chosen_bps[col] for col in t.data[0].header.values]\n",
    "    def showClouds(chosen_bps):\n",
    "        wordCloudsOutput.clear_output()\n",
    "        with wordCloudsOutput:\n",
    "            findMostCommonFAs_v2(chosen_bps, LEGEND_TITLE, WORD_CLOUD_LABEL, sort_mode=\"freq\", vis=True, n_top=4)\n",
    "    scatters = f.data\n",
    "    max_traces = len(scatters)\n",
    "    def selectBPs(trace,points,selector):\n",
    "        global indexes\n",
    "        global tr_nums\n",
    "#         print(f\"For trace index={points.trace_index} tr_nums is {tr_nums}\")\n",
    "        if not points.point_inds:\n",
    "            pass\n",
    "        else:\n",
    "            indexes.extend([trace.customdata[cur_point] for cur_point in points.point_inds])\n",
    "        tr_nums = tr_nums+1\n",
    "        if tr_nums==max_traces:\n",
    "            selected_data = filterRows(indexes)\n",
    "            updateTable(selected_data)\n",
    "            showClouds(selected_data)\n",
    "            indexes = []\n",
    "            tr_nums = 0\n",
    "    # Hover text: ID and GroundTruth\n",
    "    for scatter in scatters:\n",
    "        scatter.hoverinfo = 'text'\n",
    "        scatter.on_hover(printSignature) \n",
    "        scatter.on_click(selectBP)\n",
    "        scatter.on_selection(selectBPs)\n",
    "\n",
    "    # Selection\n",
    "    return f\n",
    "# @interact(Coloring=['label', 'GroundTruth'])\n",
    "# def update(Coloring=\"label\"):\n",
    "#     print(Coloring)\n",
    "#     f_scatter = interactiveScatter(embs, Coloring)\n",
    "#     return VBox([description, f_scatter])\n",
    "f_scatter = interactiveScatter(embs, d, LEGEND_TITLE)\n",
    "VBox([description, f_scatter, t, wordCloudsOutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# sklearn.metrics.pairwise.cosine_similarity(embs[embs.ID==10].Emb.values[0].reshape(1,-1), \n",
    "#                                            embs[embs.ID==970].Emb.values[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial hypotheis is that sub-groups of business process within a company should have some kind of cross correlation (e.g. goods delivery business process follows after sale business process). For deeper investigation of that hypothesis we are going to aggregate given Journal Entries (aka input raw data) based on the predicted cluster label and to build time-series from these groups w.r.t. to the transaction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO helper uploader for obtain Time column. \n",
    "if MODE==\"SimulatedData\":\n",
    "    df_all = d.merge(upload_JournalEntriesTruth(\"Simulation/FSN_Data.db\", limit=None)[[\"ID\", \"Time\"]], on=\"ID\")\\\n",
    "        .drop([\"Signature\", \"Name\"], axis=1)\n",
    "    print(f\"Shape of supported info is {df_all.shape}, shape of embs DataFrame is {embs.shape}\")\n",
    "    df_all = df_all.merge(embs, on=\"ID\")\n",
    "    print(f\"After merge the shape is {df_all.shape}\")\n",
    "if MODE==\"RealData\":\n",
    "    d[\"Date\"] = pd.to_datetime(d[\"Date\"],format='%Y-%m-%d')\n",
    "#     df_all = d.groupby(\"ID\", as_index=False).aggregate({\"amount\": lambda x: np.sum(x)/2.0, \n",
    "#                                \"Date\": \"first\"}).merge(embs, on=\"ID\").sort_values(\"Date\", ascending=True)\n",
    "    df_all = d.groupby([\"ID\", \"FA_Name\", \"from\"], as_index=False)\\\n",
    "                .aggregate({\"amount\": lambda x: np.sum(x), \n",
    "                            \"Date\": \"first\",\n",
    "                           \"accountDesc\": \"first\"})\\\n",
    "                .merge(embs, on=\"ID\")\\\n",
    "                .sort_values(\"Date\", ascending=True)\n",
    "    df_all.set_index(df_all.Date, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.ID.unique(), embs.ID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add DateTimeIndex to simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateTimeIndex(df, duration=\"year\", on=\"Time\"):\n",
    "    if duration==\"year\":\n",
    "        return df.set_index(df[on].apply(lambda x: np.datetime64('2019-01-01')+np.timedelta64(int(x*50), 'm')))\n",
    "if MODE==\"SimulatedData\":\n",
    "    df_all = addDateTimeIndex(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get part of data with required labe/GroundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterData(df, query=[[\"Sales 21 btw\", \"Sales 6 btw\"], [\"Collections\"]], on=\"GroundTruth\"):\n",
    "    result = list()\n",
    "    for q in query:\n",
    "        result.append(df_all[df_all[on].isin(q)])\n",
    "        result[-1].name=str(q)[1:-1]\n",
    "        if on == \"label\":\n",
    "            result[-1].name+=\" cluster\"\n",
    "    return tuple(result)\n",
    "\n",
    "def filterData_v3(df, \n",
    "                  query=[{\"select\": [\"Sales 21 btw\", \"Sales 6 btw\"], \n",
    "                              \"_with\": {\"FA_Name\": \"Revenue\", \"from\": True}}, \n",
    "                 {\"select\": [\"Collections\"], \"_with\": None}], \n",
    "                  on=\"GroundTruth\"):\n",
    "    result = list()\n",
    "    for q in query:\n",
    "        postfix = \"\"\n",
    "        cur_df = df_all[df_all[on].isin(q[\"select\"])]\n",
    "        if q[\"_with\"] is not None:\n",
    "            for key, value in q[\"_with\"].items():\n",
    "                try:\n",
    "                    cur_df = cur_df[cur_df[key]==value]\n",
    "                    postfix+=\"_\"+str(value)\n",
    "                except KeyError as e:\n",
    "                    raise(f\"{a} is not in a columns titles!\")\n",
    "        result.append(cur_df)\n",
    "        result[-1].name=str(q[\"select\"])+postfix\n",
    "        if on == \"label\":\n",
    "            result[-1].name+=\" cluster\"\n",
    "    if len(result)==1:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GroundTruth\n",
    "# sales, collections = filterData(df_all, query=[[\"Sales 21 btw\", \"Sales 6 btw\"], [\"Collections\"]])\n",
    "# Predicted labels\n",
    "# sales, collections = filterData(df_all, query=[[0], [1]], on=\"label\")\n",
    "sales, collections = filterData_v3(df_all, query=[{\"select\": [0], \n",
    "                              \"_with\": None}, \n",
    "                                {\"select\": [1], \"_with\": None}], on=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import seaborn as sns\n",
    "def plotAmounts(DFs, title=\"Default signals\", mode=\"line\"):\n",
    "    \"\"\"Helper funciton to plot a few DataFrame in one plotly graph\"\"\"\n",
    "    if mode==\"line\":\n",
    "        if len(DFs)>1:\n",
    "            fig2 = go.Figure(data=[go.Scatter(x=df.index,\n",
    "            y=df.amount,\n",
    "            name=df.name\n",
    "            ) for df in DFs], \n",
    "            layout = go.Layout(showlegend=True, title=go.layout.Title(text=title)))\n",
    "        else:\n",
    "            fig2 = go.Figure(data=go.Scatter(x=DFs.index,\n",
    "            y=DFs.amount,\n",
    "            name=DFs.name, \n",
    "            layout = go.Layout(showlegend=True, title=go.layout.Title(text=title))))\n",
    "        iplot(fig2)\n",
    "    if mode == \"scatter\" and len(DFs)==2:\n",
    "        print([df.shape for df in DFs])\n",
    "        try:\n",
    "            sns.regplot(x=DFs[0].amount, y=DFs[1].amount)\n",
    "        except:\n",
    "            pass\n",
    "#         print([df.shape for df in DFs])\n",
    "#         fig2 = go.Figure(data=[go.Scatter(x=DFs[0].amount, y=DFs[1].amount, mode='markers', name=\"Amounts\")],\n",
    "#                          layout = go.Layout(showlegend=True, \n",
    "#                                             title=go.layout.Title(text=str(DFs[0].name) +\" vs. \" + str(DFs[0].name))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAmounts([sales, collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resample our TimeSeries with Dayly/Weekly/Monthly frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_w = sales.resample(\"W\").agg({\"amount\": sum})\n",
    "sales_w.name = sales.name+\", weekly\"\n",
    "collections_w = collections.resample(\"W\").apply({\"amount\": sum})\n",
    "collections_w.name = collections.name+\", weekly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAmounts([sales_w, collections_w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(data_x, data_y, lag=0):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    datax, datay : pandas.Series objects of equal length\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    return data_x.shift(lag).corr(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[crosscorr(sales.amount.resample(\"W\").sum(), collections.amount.resample(\"W\").sum(), lag) for lag in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All in One function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "def constructSignals(df_all, query=[{\"select\": [2], \"_with\": None}, {\"select\": [4], \"_with\": None}], on=\"label\", agg_period=\"W\", mode=\"line\"):\n",
    "    # Predicted labels\n",
    "    sales, collections = filterData_v3(df_all, query=query, on=on)\n",
    "    plotAmounts([sales, collections], \"Original signals\")\n",
    "    print(\"Started aggregation...\")\n",
    "    sales_w = sales.resample(agg_period).agg({\"amount\": sum})\n",
    "    collections_w = collections.resample(agg_period).apply({\"amount\": sum})\n",
    "    if agg_period==\"W\":\n",
    "        sales_w.name = sales.name+\", weekly\"\n",
    "        collections_w.name = collections.name+\", weekly\"\n",
    "    elif agg_period==\"D\":\n",
    "        sales_w.name = sales.name+\", daily\"\n",
    "        collections_w.name = collections.name+\", daily\"\n",
    "    elif agg_period==\"M\":\n",
    "        sales_w.name = sales.name+\", monthly\"\n",
    "        collections_w.name = collections.name+\", monthly\"\n",
    "    plotAmounts([sales_w, collections_w], \"Aggregated signals\", mode=mode)\n",
    "    print([crosscorr(sales.amount.resample(agg_period).sum(), collections.amount.resample(agg_period).sum(), lag) for lag in range(4)])\n",
    "    \n",
    "#     return [sales.amount.resample(agg_period).sum(), collections.amount.resample(agg_period).sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructSignals(df_all, query=[{\"select\": [3], \n",
    "                              \"_with\": None}, \n",
    "                                {\"select\": [2], \"_with\": None}], \n",
    "                 on=\"label\", agg_period=\"M\", mode=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
