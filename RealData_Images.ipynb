{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "I advise you to create special folder - Let's call it _'AlexFinal'_ for our results, and sequentially Copy-Pasty results to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Comparison with existing methods\n",
    "\n",
    "In that subsection we demonstrate that existing methods for sampling in networks is not good for FSN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run GRID script (SensitivityAnalysis.py) for all strategies, the rect parameters set as the optimal ones:\n",
    "\n",
    "``\"Strategy\": [\"MetaUniform\", \"OriginalRandomWalk\", \"DefUniform\", \"MetaDiff\"]``\n",
    "\n",
    "As a result, we have Excel file - please copy it to _'AlexFinal'_. Also copy all images per each strategy: from OriginalRandomWalk folder, from DefUniform, from MetaUniform and from MetaDiff.\n",
    "\n",
    "Now in _'AlexFinal'_ we should have one excel file with comparison of methods AND 4 folders (one per each strategy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results interpretation\n",
    "\n",
    "In that section we make an attempt to understand how our approach works etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload your optimal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from NetEmbs import *\n",
    "\n",
    "# Specify here the number of groups in your data. \n",
    "# I'm not sure should we use the finest strucre (25) or make preelemenatry merging... What do you think?\n",
    "N_CLS = 11\n",
    "# Min and Max number of clusters to investigate the chagnes in merging processes.\n",
    "MIN_MAX_CLS = (8, 13)\n",
    "\n",
    "\n",
    "IMAGE_FOLDER = \"AlexFinal\"\n",
    "create_folder(IMAGE_FOLDER)\n",
    "\n",
    "\n",
    "MODE = \"SimulatedData\"\n",
    "DB_PATH = \"Simulation/FSN_Data_5k.db\"\n",
    "# CONFIG.ROOT_FOLDER = \"UvA/GoogleColab/Optimal 5k/\"\n",
    "CONFIG.ROOT_FOLDER = \"Uva/LargeDataset/\"\n",
    "CONFIG.EXPERIMENT = [0, 1]\n",
    "\n",
    "#  ---------- CONFIG Setting HERE ------------\n",
    "# .1 Sampling parameters\n",
    "CONFIG.STRATEGY = \"MetaDiff\"\n",
    "CONFIG.PRESSURE = 10\n",
    "CONFIG.WINDOW_SIZE = 2\n",
    "CONFIG.WALKS_PER_NODE = 30\n",
    "CONFIG.WALKS_LENGTH = 10\n",
    "# .2 TF parameters\n",
    "CONFIG.STEPS = 100000\n",
    "CONFIG.EMBD_SIZE = 8\n",
    "CONFIG.LOSS_FUNCTION = \"NegativeSampling\"  # or \"NCE\"\n",
    "CONFIG.BATCH_SIZE = 256\n",
    "CONFIG.NEGATIVE_SAMPLES = 512\n",
    "\n",
    "updateCONFIG(False)\n",
    "EMBS_PATH = CONFIG.WORK_FOLDER[0]+CONFIG.WORK_FOLDER[1]+CONFIG.WORK_FOLDER[2]\n",
    "\n",
    "if MODE == \"SimulatedData\":\n",
    "    embs = pd.read_pickle(EMBS_PATH+\"Embeddings.pkl\")\n",
    "    print(\"Embeddings have been uploaded to memory!\")\n",
    "    d = upload_data(DB_PATH, limit=None)\n",
    "    d = prepare_data(d)\n",
    "    print(\"Supported information has been uploaded to memory!\")\n",
    "\n",
    "if MODE == \"RealData\":\n",
    "    import extras\n",
    "    import analysis\n",
    "\n",
    "    embs = pd.read_pickle(EMBS_PATH+\"Embeddings.pkl\")\n",
    "        # //////// TODO UPLOAD your data HERE \\\\\\\\\\\\\\\\\\\\\n",
    "#     d = analysis.analysis(\"14082_2017\")\n",
    "    d = extras.ballData()\n",
    "    d = rename_columns(d, names={\"transactionID\": \"ID\", \"accountID\": \"FA_Name\", \"BR\": \"GroundTruth\",\n",
    "                                  \"amount\": \"Value\", \"date\": \"Date\", \"account\" : \"accountDesc\"})\n",
    "        # //////// END  \\\\\\\\\\\\\\\\\\\\\n",
    "    # TODO pay attention for the split argument below!\n",
    "    if \"Value\" in list(d):\n",
    "        need_split = True\n",
    "    else:\n",
    "        need_split = False\n",
    "    d = prepare_dataMarcel(d, split=need_split)\n",
    "#     Here we drop the duplicate of GroundTruth in the DataFrame with supported info, because we have it in Embs DataFrame\n",
    "    if \"GroundTruth\" in list(d):\n",
    "        d.drop(\"GroundTruth\", axis=1, inplace=True)\n",
    "print(\"Shape of uploaded Dataset is \", d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterize here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl_n in range(*MIN_MAX_CLS):\n",
    "    embs = cl_Agglomerative(embs, cl_n)\n",
    "    embs.groupby([\"GroundTruth\", \"label\"])[\"ID\"].count()\\\n",
    "        .unstack().fillna(0).apply(lambda x: x / x.sum(), axis = 1)\\\n",
    "        .to_excel(f\"{IMAGE_FOLDER}/confusion{str(cl_n)}.xlsx\")\n",
    "    plot_tSNE(embs, legend_title=\"label\", title=IMAGE_FOLDER+\"/predicted_MetaDiff_\"+str(cl_n), context=\"talk_half\")\n",
    "    print(f\"Done with {cl_n} clusters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = cl_Agglomerative(embs, N_CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = cl_Agglomerative(embs, 14)\n",
    "plot_tSNE(embs, legend_title=\"label\", title=IMAGE_FOLDER+\"/predicted14_MetaDiff\", context=\"talk_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncovering the actual meaning\n",
    "\n",
    "If I understood you correctly, it is impossible to post the same images as you send to your colleagues. So, let's just add to our thesis to that section their feedback, I guess it should be OK as well as demonstrates that we actively communicate with experts from the field of study :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# Standard plotly imports\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, plot, init_notebook_mode\n",
    "# Using plotly + cufflinks in offline mode\n",
    "import cufflinks\n",
    "\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)\n",
    "from ipywidgets import interactive, HBox, VBox, widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive vis for you to undersand the meaning of cluster for further modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from NetEmbs.Vis.helpers import set_font, getColors_Markers\n",
    "from NetEmbs.Vis.words_cloud import findMostCommonFAs_v2\n",
    "# Label text\n",
    "description = widgets.Label(\n",
    "        value=''\n",
    "    )\n",
    "# WordCouds area\n",
    "wordCloudsOutput = widgets.Output()\n",
    "# Table with JournalEntries data\n",
    "table_titles = [\"ID\", \"FA_Name\", \"Credit\", \"Debit\", \"label\"]\n",
    "format_style = [[None], [None], ['.4f'], ['.4f'], [None]]\n",
    "\n",
    "if MODE == \"RealData\":\n",
    "    table_titles = [\"ID\", \"FA_Name\", \"accountDesc\", \"Credit\", \"Debit\", \"label\"]\n",
    "    format_style = [[None], [None], [None], ['.4f'], ['.4f'], [None]]\n",
    "\n",
    "t = go.FigureWidget([go.Table(\n",
    "    header=dict(values=table_titles,\n",
    "                fill = dict(color='#E5F1DC'),\n",
    "                align = ['center'] * 5),\n",
    "    cells=dict(values=[],\n",
    "               format = format_style,\n",
    "#                fill = dict(color='white'),\n",
    "               align = ['center'] * 5))],\n",
    "                    layout = go.Layout(\n",
    "                            title=\"Journal Entries\",\n",
    "                            autosize=True,\n",
    "                            width=1000,\n",
    "                            height=400))  \n",
    "# Scatter plot\n",
    "N_COLORS = 10\n",
    "WORD_CLOUD_LABEL = \"FA_Name\"\n",
    "if MODE == \"RealData\":\n",
    "    WORD_CLOUD_LABEL = \"accountDesc\"\n",
    "LEGEND_TITLE = \"GroundTruth\"\n",
    "# LEGEND_TITLE = \"label\"\n",
    "\n",
    "tmp_p_see = None\n",
    "# For selection via multiple traces... stupid way.\n",
    "indexes = []\n",
    "tr_nums = 0\n",
    "\n",
    "\n",
    "def interactiveScatter(df, df_info, legend_title=\"label\"):\n",
    "    \"\"\"Create FigureWidget with the scatter plot for the given DataFrame\"\"\"\n",
    "    scatter_data = list()\n",
    "    cmap, mmap = getColors_Markers(keys=df[legend_title].unique(), cm=\"tab10\", n_colors=N_COLORS, markers=[\"circle\", \"diamond\", \"square\"])\n",
    "    for name, group in df.groupby(legend_title):\n",
    "        scatter_data.append(go.Scatter(x=group.x, y=group.y, mode='markers', name=name, \n",
    "                                    text = group.apply(lambda row: f\"ID={row.ID},   GroundTruth={row.GroundTruth}\", axis=1),\n",
    "                                    customdata = group.index.to_list(),\n",
    "                                    marker=dict(color=cmap[name][1], \n",
    "                                                symbol=mmap[name])))\n",
    "    f = go.FigureWidget(data=scatter_data,\n",
    "                       layout = go.Layout(\n",
    "                           title=f\"t-SNE visualisation with coloring based on {legend_title}\",\n",
    "        hovermode='closest',\n",
    "        autosize=True,\n",
    "        width=1000,\n",
    "        height=700))\n",
    "    \n",
    "    def printSignature(trace, points, *args):\n",
    "        if len(points.point_inds)>0:\n",
    "            ids = trace.customdata[points.point_inds[0]]\n",
    "            row = df.iloc[ids]\n",
    "            description.value = f\"ID={row.ID},   GroundTruth={row.GroundTruth}\"\n",
    "    def selectBP(trace, points, *args):\n",
    "        if len(points.point_inds)>0:\n",
    "            ids = trace.customdata[points.point_inds[0]]\n",
    "            row = df.iloc[[ids]]\n",
    "            chosen_bps = df_info.merge(row, on=\"ID\")\n",
    "            wordCloudsOutput.clear_output()\n",
    "            t.data[0].cells.values = [chosen_bps[col] for col in t.data[0].header.values]\n",
    "    \n",
    "    def filterRows(selected_ids):\n",
    "        row = df.iloc[selected_ids]\n",
    "        chosen_bps = df_info.merge(row, on=\"ID\")\n",
    "        return chosen_bps\n",
    "    def updateTable(chosen_bps):\n",
    "        t.data[0].cells.values = [chosen_bps[col] for col in t.data[0].header.values]\n",
    "    def showClouds(chosen_bps):\n",
    "        wordCloudsOutput.clear_output()\n",
    "        with wordCloudsOutput:\n",
    "            findMostCommonFAs_v2(chosen_bps, LEGEND_TITLE, WORD_CLOUD_LABEL, sort_mode=\"freq\", vis=True, n_top=4)\n",
    "    scatters = f.data\n",
    "    max_traces = len(scatters)\n",
    "    def selectBPs(trace,points,selector):\n",
    "        global indexes\n",
    "        global tr_nums\n",
    "#         print(f\"For trace index={points.trace_index} tr_nums is {tr_nums}\")\n",
    "        if not points.point_inds:\n",
    "            pass\n",
    "        else:\n",
    "            indexes.extend([trace.customdata[cur_point] for cur_point in points.point_inds])\n",
    "        tr_nums = tr_nums+1\n",
    "        if tr_nums==max_traces:\n",
    "            selected_data = filterRows(indexes)\n",
    "            updateTable(selected_data)\n",
    "            showClouds(selected_data)\n",
    "            indexes = []\n",
    "            tr_nums = 0\n",
    "    # Hover text: ID and GroundTruth\n",
    "    for scatter in scatters:\n",
    "        scatter.hoverinfo = 'text'\n",
    "        scatter.on_hover(printSignature) \n",
    "        scatter.on_click(selectBP)\n",
    "        scatter.on_selection(selectBPs)\n",
    "\n",
    "    # Selection\n",
    "    return f\n",
    "# @interact(Coloring=['label', 'GroundTruth'])\n",
    "# def update(Coloring=\"label\"):\n",
    "#     print(Coloring)\n",
    "#     f_scatter = interactiveScatter(embs, Coloring)\n",
    "#     return VBox([description, f_scatter])\n",
    "f_scatter = interactiveScatter(embs, d, LEGEND_TITLE)\n",
    "VBox([description, f_scatter, t, wordCloudsOutput])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling graphs\n",
    "\n",
    "Install additional packages for saving figures from Plotly:\n",
    "\n",
    "``conda install -c plotly plotly-orca psutil requests``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO helper uploader for obtain Time column. \n",
    "if MODE==\"SimulatedData\":\n",
    "    df_all = d.drop([\"Signature\", \"Name\"], axis=1)\n",
    "    print(f\"Shape of supported info is {df_all.shape}, shape of embs DataFrame is {embs.shape}\")\n",
    "    df_all = df_all.merge(embs, on=\"ID\")\n",
    "    print(f\"After merge the shape is {df_all.shape}\")\n",
    "    df_all = df_all.groupby([\"ID\", \"FA_Name\", \"from\"], as_index=False).aggregate({\"amount\": lambda x: np.sum(x), \n",
    "                                \"Time\": \"first\",\n",
    "                              \"GroundTruth\": \"first\",\n",
    "                              \"label\": \"first\",\n",
    "                              \"x\": \"first\",\n",
    "                              \"y\": \"first\"}) \\\n",
    "                        .sort_values(\"Time\", ascending=True)\n",
    "if MODE==\"RealData\":\n",
    "    d[\"Date\"] = pd.to_datetime(d[\"Date\"],format='%Y-%m-%d')\n",
    "#     df_all = d.groupby(\"ID\", as_index=False).aggregate({\"amount\": lambda x: np.sum(x)/2.0, \n",
    "#                                \"Date\": \"first\"}).merge(embs, on=\"ID\").sort_values(\"Date\", ascending=True)\n",
    "    df_all = d.groupby([\"ID\", \"FA_Name\", \"from\"], as_index=False)\\\n",
    "                .aggregate({\"amount\": lambda x: np.sum(x), \n",
    "                            \"Date\": \"first\",\n",
    "                           \"accountDesc\": \"first\"})\\\n",
    "                .merge(embs, on=\"ID\")\\\n",
    "                .sort_values(\"Date\", ascending=True)\n",
    "    df_all.set_index(df_all.Date, inplace=True)\n",
    "df_all[\"flow\"] = df_all[\"from\"].apply(lambda x: {False: \"inflow\", True: \"outflow\" }[x])\n",
    "df_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDateTimeIndex(df):\n",
    "    df[\"SimulatedTime\"] = df[\"Time\"]\n",
    "    df[\"Time\"] = df[\"Time\"].apply(lambda x: np.datetime64('2019-01-01')+np.timedelta64(int(x*205.7), 'm'))\n",
    "    return df.set_index(\"Time\")\n",
    "if MODE==\"SimulatedData\":\n",
    "    df_all = addDateTimeIndex(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterData_v3(df, \n",
    "                  query=[{\"select\": [\"Sales 21 btw\", \"Sales 6 btw\"], \n",
    "                              \"_with\": {\"FA_Name\": \"Revenue\", \"from\": True}}, \n",
    "                 {\"select\": [\"Collections\"], \"_with\": None}], \n",
    "                  on=\"GroundTruth\"):\n",
    "    result = list()\n",
    "    for q in query:\n",
    "        postfix = {\"FA_Name\": None, \"accountDesc\": None, \"flow\": None}\n",
    "        if q[\"select\"] is None or q[\"select\"]==[\"ALL\"] or q[\"select\"]==\"ALL\":\n",
    "            cur_df = df\n",
    "            cur_df.name = \"No processes information\"\n",
    "        else:\n",
    "            cur_df = df[df[on].isin(q[\"select\"])]\n",
    "            if on==\"GroundTruth\":\n",
    "                cur_df.name = \"Expert label\"\n",
    "                if len(q[\"select\"])>1:\n",
    "                        cur_df.name+=\"s\"\n",
    "            elif on == \"label\":\n",
    "                cur_df.name = \"Cluster\"\n",
    "                if len(q[\"select\"])>1:\n",
    "                        cur_df.name+=\"s\"\n",
    "            cur_df.name+=\" \"+str(q[\"select\"])[1:-1]\n",
    "        tmp_name = cur_df.name\n",
    "        if q[\"_with\"] is not None:\n",
    "            for key, value in q[\"_with\"].items():\n",
    "                try:\n",
    "                    cur_df = cur_df[cur_df[key]==value]\n",
    "                    postfix[key] = str(value)\n",
    "                except KeyError as e:\n",
    "                    raise(f\"{a} is not in a columns titles!\")\n",
    "    \n",
    "        if postfix[\"FA_Name\"] is not None:\n",
    "            tmp_name+=f\" – {postfix['FA_Name']}\"\n",
    "        elif postfix[\"accountDesc\"] is not None:\n",
    "            tmp_name+=f\" – {postfix['accountDesc']}\"\n",
    "        if postfix[\"flow\"] is not None:\n",
    "            tmp_name+=f\"({postfix['flow']})\"\n",
    "        cur_df.name = tmp_name\n",
    "        result.append(cur_df)\n",
    "    if len(result)==1:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return tuple(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the ~map from Label to GroundTruth, but it could be wrong, because I simply take the most common GroundTruth for each predicted label. But, at least, it can help you I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_titles = embs.groupby(\"label\").GroundTruth.agg(pd.Series.mode).to_dict()\n",
    "labels_to_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint! To make plots applicable for publication, I hide Y axis, now one does not know the values of monetary flow. Is it enough? \n",
    "\n",
    "Keep in mind, that if we change the amounts values or shift original Xaxis, it leads to changes in correlation between Time series (that was the first one I had tried...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import numpy as np\n",
    "def plotAmounts(DFs, aggtitle=\"Default signals\", corr_score=None, filename=\"amounts_plot\", anonym=False):\n",
    "    \"\"\"Helper funciton to plot a few DataFrame in one plotly graph\"\"\"\n",
    "    if corr_score is not None:\n",
    "        aggtitle = aggtitle + \".     Correlation: \"+str(round(corr_score, 3))\n",
    "    if len(DFs)>1:\n",
    "            fig2 = go.Figure(data=[go.Scatter(x=df.index,\n",
    "            y=df.amount,\n",
    "            name=df.name\n",
    "            ) for df in DFs], \n",
    "            layout = go.Layout(width=1200,\n",
    "    height=400, showlegend=True, title=go.layout.Title(text=aggtitle), hovermode='closest', \n",
    "                               legend=dict(orientation=\"h\", font=dict(size=18), xanchor='center', x=0.5, y=-0.1),\n",
    "                              font=dict(size=18)))\n",
    "    else:\n",
    "            fig2 = go.Figure(data=go.Scatter(x=DFs.index,\n",
    "            y=DFs.amount,\n",
    "            name=DFs.name, \n",
    "            layout = go.Layout(width=1200,\n",
    "            height=400, showlegend=True, title=go.layout.Title(text=aggtitle), hovermode='closest')))\n",
    "    if anonym:\n",
    "        fig2.layout.yaxis = go.layout.YAxis(showticklabels=False)\n",
    "        fig2.layout.xaxis = go.layout.XAxis(showticklabels=False)\n",
    "    iplot(fig2)\n",
    "    fig2.write_image(filename+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error\n",
    "\n",
    "legend_postfix = {\"W\": \", weekly\", \"D\": \", daily\", \"M\": \", monthly\", \"2D\": \", 2 days\"}\n",
    "def constructSignals_v2(df_all, shift=(0, 0), query=[{\"select\": [2], \"_with\": None}, {\"select\": [4], \"_with\": None}],\n",
    "                        on=\"label\", agg_period=\"W\", title=None, legend=None, scale_data=True):\n",
    "    # Predicted labels\n",
    "    agg_title = \"Aggregated signals\"\n",
    "    left, right = filterData_v3(df_all, query=query, on=on)\n",
    "    #     Make required shifts\n",
    "    left_agg = left.shift(shift[0], freq=\"D\")\n",
    "    right_agg = right.shift(shift[1], freq=\"D\")\n",
    "    #     Aling TimeIndexes for correct aggregation.\n",
    "    st_date = max(left_agg.index[0], right_agg.index[0])\n",
    "    end_date = min(left_agg.index[-1], right_agg.index[-1])\n",
    "    left_agg = left_agg[(left_agg.index >= st_date) & (left_agg.index <= end_date)]\n",
    "    right_agg = right_agg[(right_agg.index >= st_date) & (right_agg.index <= end_date)]\n",
    "    #     Makre required aggregation\n",
    "    left_agg = left_agg.resample(agg_period).agg({\"amount\": sum})\n",
    "    right_agg = right_agg.resample(agg_period).apply({\"amount\": sum})\n",
    "    if legend is not None:\n",
    "        left_agg.name = legend[0] + legend_postfix[agg_period]\n",
    "        right_agg.name = legend[1] + legend_postfix[agg_period]\n",
    "        agg_title += legend_postfix[agg_period]\n",
    "    #     Add info about aggregation period to legen texts\n",
    "    else:\n",
    "        try:\n",
    "            left_agg.name = left.name + legend_postfix[agg_period]\n",
    "            right_agg.name = right.name + legend_postfix[agg_period]\n",
    "            agg_title += legend_postfix[agg_period]\n",
    "        except KeyError as e:\n",
    "            left_agg.name = left.name + \", \" + agg_period\n",
    "            right_agg.name = right.name + \", \" + agg_period\n",
    "            agg_title += \", \" + agg_period\n",
    "    print(f\"Correlation for given query and given shifts {shift} is \\\n",
    "          {left_agg.amount.corr(right_agg.amount)}\")\n",
    "\n",
    "    all_data = left_agg.join(right_agg, lsuffix=\"_X\", rsuffix=\"_Y\", how=\"inner\")\n",
    "\n",
    "    plotAmounts([left_agg, right_agg], agg_title, filename=title + \"_\" + agg_period,\n",
    "                corr_score=left_agg.amount.corr(right_agg.amount), anonym=True)\n",
    "    if scale_data:\n",
    "        from sklearn.preprocessing import minmax_scale\n",
    "        all_data[\"amount_Y\"] = minmax_scale(all_data[\"amount_Y\"])\n",
    "        all_data[\"amount_X\"] = minmax_scale(all_data[\"amount_X\"])\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.divide(np.abs((y_true - y_pred)), y_true, out=np.zeros_like(y_true), where=y_true!=0)) * 100\n",
    "\n",
    "def NRMSD(true_labels, pred_labels):\n",
    "    mse = mean_squared_error(true_labels, pred_labels)\n",
    "    return np.sqrt(mse) / (np.subtract(*np.percentile(true_labels, [75, 25])))\n",
    "\n",
    "\n",
    "def evaluate_model(df, metric=\"RMSE\", n_runs=5):\n",
    "    scores = list()\n",
    "    lr = LinearRegression()\n",
    "    if \"amount_X\" not in list(df) or \"amount_Y\" not in list(df):\n",
    "        raise KeyError(f\"Could not find the columns with X and Y in the given dataset... Titles are {list(df)}, while \"\n",
    "                       f\"'amount_X' and 'amount_Y' required!\")\n",
    "    for r_s in range(n_runs):\n",
    "        #         Make new split\n",
    "        train, test = train_test_split(df, test_size=0.2, random_state=r_s)\n",
    "        lr.fit(train.iloc[:, 0].values.reshape(-1, 1), train.iloc[:, 1].values.reshape(-1, 1))\n",
    "        #         print(\"Coefficients in constructed linear regression model are: :\", lr.coef_)\n",
    "        tax_predicted = lr.predict(test[[\"amount_X\"]])\n",
    "        if metric == \"MSE\":\n",
    "            scores.append(mean_squared_error(test[['amount_Y']], tax_predicted))\n",
    "        elif metric == \"RMSE\":\n",
    "            scores.append(np.sqrt(mean_squared_error(test[['amount_Y']], tax_predicted)))\n",
    "        elif metric == \"NRMSD\":\n",
    "            scores.append(NRMSD(test[['amount_Y']], tax_predicted))\n",
    "        elif metric == \"MAPE\":\n",
    "            scores.append(MAPE(test[['amount_Y']], tax_predicted))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for regression plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRegression(df, context=\"talk_full\", labels = [\"Outflow from Revenue\", \"Outflow from Tax\"], title=None):\n",
    "    dpi = 140\n",
    "    fig_size = (11, 11)\n",
    "    #     set_font(14, True)\n",
    "    if context == \"paper_half\":\n",
    "        sns.set_context(\"paper\", font_scale=1.5)\n",
    "        fig_size = (6.4, 4.8)\n",
    "    if context == \"paper_full\":\n",
    "        sns.set_context(\"paper\", font_scale=1.8)\n",
    "    if context == \"talk_half\":\n",
    "        sns.set_context(\"paper\", font_scale=3.5)\n",
    "    if context == \"talk_full\":\n",
    "        sns.set_context(\"paper\", font_scale=2)\n",
    "    cur_plot = sns.lmplot(x=\"amount_X\", y=\"amount_Y\", col=\"Model\", data=df, col_wrap=2, height=5, aspect=1.5, scatter_kws=dict(s=70, alpha=0.75), line_kws=dict(alpha=0.8))\\\n",
    "        .set_titles(col_template = '{col_name}')\\\n",
    "        .set_xlabels(labels[0])\\\n",
    "        .set_ylabels(labels[1])\\\n",
    "        .set(xlim=(0, 1), ylim=(0, 1))\n",
    "    cur_plot.fig.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "    if title is not None:\n",
    "        cur_plot.savefig(title + \"RegressionPlot_for_\" + context + \".png\", bbox_inches=\"tight\", dpi=dpi, pad_inches=0.05)\n",
    "    plt.show(cur_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your experiments with relationship\n",
    "\n",
    "1. Start from CD model, where we do not use processes info -> `\"select\": [\"ALL\"]`\n",
    "2. Then move to model based on the GroundTruth\n",
    "3. And finally, use predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship sales dispatch\n",
    "BR5_cost.of.sales_debit ~ BR4_revenue_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Relationship_name = \"SalesDispatch\"\n",
    "TAX_REVENUE_DF = pd.DataFrame() # Empty DataFrame to store results\n",
    "# /////////// CD Model query \\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "CURENT_QUERY = [{\"select\": [\"ALL\"], \n",
    "                              \"_with\": {\"accountDesc\": \"Cost.of.sales\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [\"ALL\"], \"_with\": {\"accountDesc\": \"Revenue\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"label\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/CD_Model_{Relationship_name}_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"CD Model\", agg_period=agg_period))\n",
    "    \n",
    "# Repeat the same for expert labels\n",
    "# ///////// Expert Model query \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "CURENT_QUERY = [{\"select\": [\"BR5\"], \n",
    "                              \"_with\": {\"accountDesc\": \"Cost.of.sales\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [\"BR4\"], \"_with\": {\"accountDesc\": \"Revenue\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"GroundTruth\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/expertFSN_{Relationship_name}_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"Expert labels FSN Model\", agg_period=agg_period))\n",
    "    \n",
    "# And predicted labels\n",
    "# CD Model query\n",
    "CURENT_QUERY = [{\"select\": [4], \n",
    "                              \"_with\": {\"accountDesc\": \"Cost.of.sales\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [2], \"_with\":  {\"accountDesc\": \"Revenue\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"label\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/aFSN_Tax-Revenue_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"Predicted labels FSN Model\", agg_period=agg_period))\n",
    "    \n",
    "# Store scores to file\n",
    "TAX_REVENUE_DF.groupby([\"Model\", \"agg_period\"]).apply(evaluate_model).to_excel(f\"{IMAGE_FOLDER}/Scores_for_{Relationship_name}.xlsx\")\n",
    "\n",
    "# Plot regression, Do not worry, I scaled the data here to unit interval. Update Axis labels according to your data.\n",
    "for name, group in TAX_REVENUE_DF.groupby(\"agg_period\"):\n",
    "    plotRegression(group, title=f\"{IMAGE_FOLDER}/{Relationship_name}_{name}\", labels=[\"Inflow to Cost.of.sales\", \"Outflow from Revenue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship dispatch\n",
    "We expect that cost of goods sold and inventories will be related and in this case more or less equal to each other.\n",
    "\n",
    "BR5_inventories_credit ~ BR5_Cost.of.sales_credit\n",
    "\n",
    "-----\n",
    "! I guess it should be \n",
    "BR5_inventories_credit ~ BR5_Cost.of.sales_debit ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Relationship_name = \"Dispatch\"\n",
    "TAX_REVENUE_DF = pd.DataFrame() # Empty DataFrame to store results\n",
    "# /////////// CD Model query \\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "CURENT_QUERY = [{\"select\": [\"ALL\"], \n",
    "                              \"_with\": {\"accountDesc\": \"Cost.of.sales\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [\"ALL\"], \"_with\": {\"accountDesc\": \"Inventories\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"label\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/CD_Model_Tax-Revenue_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"CD Model\", agg_period=agg_period))\n",
    "    \n",
    "# Repeat the same for expert labels\n",
    "# ///////// Expert Model query \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "CURENT_QUERY = [{\"select\": [\"BR5\"], \n",
    "                              \"_with\": {\"accountDesc\": \"Cost.of.sales\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [\"BR5\"], \"_with\": {\"accountDesc\": \"Inventories\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"GroundTruth\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/expertFSN_Tax-Revenue_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"Expert labels FSN Model\", agg_period=agg_period))\n",
    "    \n",
    "# And predicted labels\n",
    "# CD Model query\n",
    "CURENT_QUERY = [{\"select\": [4], \n",
    "                              \"_with\": {\"accountDesc\": \"Cost.of.sales\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [4], \"_with\":  {\"accountDesc\": \"Inventories\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"label\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/aFSN_Tax-Revenue_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"Predicted labels FSN Model\", agg_period=agg_period))\n",
    "    \n",
    "# Store scores to file\n",
    "TAX_REVENUE_DF.groupby([\"Model\", \"agg_period\"]).apply(evaluate_model).to_excel(f\"{IMAGE_FOLDER}/Scores_for_{Relationship_name}.xlsx\")\n",
    "\n",
    "# Plot regression, Do not worry, I scaled the data here to unit interval. Update Axis labels according to your data.\n",
    "for name, group in TAX_REVENUE_DF.groupby(\"agg_period\"):\n",
    "    plotRegression(group, title=f\"{IMAGE_FOLDER}/{Relationship_name}_{name}\", labels=[\"Inflow to Cost.of.sales\", \"Outflow from Inventories\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship purchases and goods received\n",
    "We expect that the company purcheses goods and in the end receive the goods therefore we expect that the following holds:\n",
    "\n",
    "BR7_inventories.suspense.account_credit ~ BR6_Inventories.suspense.account_debit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Relationship_name = \"PurchasesAndGoodsReceived\"\n",
    "TAX_REVENUE_DF = pd.DataFrame() # Empty DataFrame to store results\n",
    "# /////////// CD Model query \\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "CURENT_QUERY = [{\"select\": [\"ALL\"], \n",
    "                              \"_with\": {\"accountDesc\": \"Inventories.suspense.account\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [\"ALL\"], \"_with\": {\"accountDesc\": \"Inventories.suspense.account\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"label\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/CD_Model_Tax-Revenue_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"CD Model\", agg_period=agg_period))\n",
    "    \n",
    "# Repeat the same for expert labels\n",
    "# ///////// Expert Model query \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "CURENT_QUERY = [{\"select\": [\"BR6\"], \n",
    "                              \"_with\": {\"accountDesc\": \"Inventories.suspense.account\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [\"BR7\"], \"_with\": {\"accountDesc\": \"Inventories.suspense.account\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"GroundTruth\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/expertFSN_Tax-Revenue_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"Expert labels FSN Model\", agg_period=agg_period))\n",
    "    \n",
    "# And predicted labels\n",
    "# Predicted Model query\n",
    "CURENT_QUERY = [{\"select\": [3], \n",
    "                              \"_with\": {\"accountDesc\": \"Inventories.suspense.account\", \"flow\": \"inflow\"}}, \n",
    "                                {\"select\": [5], \"_with\":  {\"accountDesc\": \"Inventories.suspense.account\", \"flow\": \"outflow\"}}]\n",
    "for agg_period in [\"2D\", \"W\", \"M\"]:\n",
    "    cur_result = constructSignals_v2(df_all, query=CURENT_QUERY, \n",
    "                                 on=\"label\", agg_period=agg_period, \n",
    "                title=f\"{IMAGE_FOLDER}/aFSN_Tax-Revenue_{agg_period}_\")\n",
    "    TAX_REVENUE_DF = TAX_REVENUE_DF.append(cur_result.assign(Model=\"Predicted labels FSN Model\", agg_period=agg_period))\n",
    "    \n",
    "# Store scores to file\n",
    "TAX_REVENUE_DF.groupby([\"Model\", \"agg_period\"]).apply(evaluate_model).to_excel(f\"{IMAGE_FOLDER}/Scores_for_{Relationship_name}.xlsx\")\n",
    "\n",
    "# Plot regression, Do not worry, I scaled the data here to unit interval. Update Axis labels according to your data.\n",
    "for name, group in TAX_REVENUE_DF.groupby(\"agg_period\"):\n",
    "    plotRegression(group, title=f\"{IMAGE_FOLDER}/{Relationship_name}_{name}\", labels=[\"Inflow to Inventories.suspense\", \"Outflow from Inventories.suspense\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
